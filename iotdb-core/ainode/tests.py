#!/usr/bin/env python3
# test_timesfm_integration.py

import os
import sys
import tempfile
import shutil
import json
import yaml
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# æ¨¡æ‹ŸAINodeç¯å¢ƒ
class MockLogger:
    """æ¨¡æ‹ŸLoggerç±»ï¼Œä¸çœŸå®Loggeræ¥å£å…¼å®¹"""
    def __init__(self, name=None):
        self.name = name or "mock_logger"
    
    def debug(self, msg, *args, **kwargs): 
        print(f"DEBUG: {msg}")
    
    def info(self, msg, *args, **kwargs): 
        print(f"INFO: {msg}")
    
    def warning(self, msg, *args, **kwargs): 
        print(f"WARNING: {msg}")
    
    def error(self, msg, *args, **kwargs): 
        print(f"ERROR: {msg}")
    
    def critical(self, msg, *args, **kwargs): 
        print(f"CRITICAL: {msg}")

class MockAINodeConfig:
    def get_ain_models_dir(self):
        return "data/ainode/models"
    
    def get_ain_builtin_models_dir(self):
        return "data/ainode/models/weights"

class MockAINodeDescriptor:
    def get_config(self):
        return MockAINodeConfig()

# è®¾ç½®ç¯å¢ƒ
os.environ['AINODE_TEST'] = 'true'

# æ¨¡æ‹Ÿä¾èµ–æ¨¡å—
sys.modules['ainode.core.log'] = type('MockLogModule', (), {
    'Logger': MockLogger
})()

sys.modules['ainode.core.config'] = type('MockConfigModule', (), {
    'AINodeDescriptor': MockAINodeDescriptor
})()

# æ¨¡æ‹Ÿå…¶ä»–å¯èƒ½çš„ä¾èµ–
class MockTTTypes:
    """æ¨¡æ‹Ÿthriftç±»å‹"""
    pass

sys.modules['ainode.thrift.common.ttypes'] = MockTTTypes()
sys.modules['ainode.thrift.ainode.ttypes'] = MockTTTypes()

# æ£€æŸ¥transformersç‰ˆæœ¬
try:
    import transformers
    print(f"Transformers version: {transformers.__version__}")
except ImportError:
    print("Transformers not installed")
    sys.exit(1)

# å¯¼å…¥TimesFMæ¨¡å—
try:
    # å¯¼å…¥é…ç½®ç±»
    from ainode.core.model.timesfm.configuration_timesfm import TimesFmConfig
    print("âœ“ Successfully imported TimesFmConfig")
    
    # å¯¼å…¥ç”Ÿæˆæ··åˆç±»
    from ainode.core.model.timesfm.timesfm_generation_mixin import TimesFmGenerationMixin
    print("âœ“ Successfully imported TimesFmGenerationMixin")
    
    # å¯¼å…¥æ¨¡å‹ç±»
    from ainode.core.model.timesfm.modeling_timesfm import (
        TimesFmForPrediction, 
        TimesFmOutput, 
        TimesFmOutputForPrediction
    )
    print("âœ“ Successfully imported TimesFM model classes")
    
except ImportError as e:
    print(f"âœ— Import error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

class TimesFMIntegrationTest:
    def __init__(self):
        self.test_dir = tempfile.mkdtemp(prefix="timesfm_test_")
        self.model_dir = os.path.join(self.test_dir, "timesfm_model")
        os.makedirs(self.model_dir, exist_ok=True)
        print(f"Test directory: {self.test_dir}")

    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç›®å½•"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
        print("âœ“ Cleaned up test directory")

    def test_config_creation(self):
        """æµ‹è¯•TimesFmConfigåˆ›å»º"""
        print("\n--- Testing TimesFmConfig Creation ---")
        try:
            # æµ‹è¯•é»˜è®¤é…ç½® - ä¸ä¼ é€’å¯èƒ½æœ‰é—®é¢˜çš„å‚æ•°
            print("  - Creating default config...")
            config = TimesFmConfig()
            print(f"âœ“ Default TimesFmConfig created successfully")
            print(f"  - Model type: {config.model_type}")
            print(f"  - Patch length: {config.patch_length}")
            print(f"  - Context length: {config.context_length}")
            print(f"  - Horizon length: {config.horizon_length}")
            print(f"  - Hidden size: {config.hidden_size}")
            
            # éªŒè¯é…ç½®çš„åŸºæœ¬å±æ€§
            assert hasattr(config, 'patch_length'), "Config missing patch_length"
            assert hasattr(config, 'context_length'), "Config missing context_length"
            assert hasattr(config, 'horizon_length'), "Config missing horizon_length"
            assert hasattr(config, 'hidden_size'), "Config missing hidden_size"
            assert hasattr(config, 'num_attention_heads'), "Config missing num_attention_heads"
            
            # æµ‹è¯•è‡ªå®šä¹‰é…ç½® - åªä¼ é€’TimesFMç‰¹æœ‰çš„å‚æ•°
            print("  - Creating custom config...")
            custom_config = TimesFmConfig(
                patch_length=16,
                context_length=256,
                horizon_length=32,
                hidden_size=128,
                num_hidden_layers=2,
                num_attention_heads=4,
                head_dim=32
            )
            print(f"âœ“ Custom TimesFmConfig created successfully")
            print(f"  - Custom patch length: {custom_config.patch_length}")
            print(f"  - Custom hidden size: {custom_config.hidden_size}")
            print(f"  - Custom layers: {custom_config.num_hidden_layers}")
            
            # éªŒè¯è‡ªå®šä¹‰å‚æ•°
            assert custom_config.patch_length == 16, f"Expected 16, got {custom_config.patch_length}"
            assert custom_config.hidden_size == 128, f"Expected 128, got {custom_config.hidden_size}"
            
            # æµ‹è¯•é€šè¿‡å…³é”®å­—å‚æ•°ä¼ é€’transformersæ ‡å‡†å‚æ•°
            print("  - Creating config with transformers kwargs...")
            kwargs_config = TimesFmConfig(
                patch_length=16,
                context_length=128,
                hidden_size=64,
                use_cache=True,  # é€šè¿‡kwargsä¼ é€’
                output_attentions=False,  # é€šè¿‡kwargsä¼ é€’
            )
            print(f"âœ“ Config with kwargs created successfully")
            
            return True
        except Exception as e:
            print(f"âœ— Error creating TimesFmConfig: {e}")
            import traceback
            traceback.print_exc()
            return False

    def test_timesfm_model_creation(self):
        """æµ‹è¯•TimesFMæ¨¡å‹åˆ›å»º"""
        print("\n--- Testing TimesFM Model Creation ---")
        try:
            # ä½¿ç”¨è¾ƒå°çš„é…ç½®è¿›è¡Œæµ‹è¯•ï¼Œé¿å…ä¼ é€’æœ‰é—®é¢˜çš„å‚æ•°
            config = TimesFmConfig(
                patch_length=16,
                context_length=256,
                horizon_length=32,
                hidden_size=128,
                intermediate_size=128,
                num_hidden_layers=2,
                num_attention_heads=4,
                head_dim=32,
                freq_size=3
            )
            
            print(f"  - Creating model with config: patch_len={config.patch_length}, "
                  f"hidden_size={config.hidden_size}, layers={config.num_hidden_layers}")
            
            model = TimesFmForPrediction(config)
            print(f"âœ“ TimesFM model created successfully")
            print(f"  - Model type: {type(model).__name__}")
            print(f"  - Context length: {model.context_len}")
            print(f"  - Horizon length: {model.horizon_len}")
            
            # éªŒè¯æ¨¡å‹ç»“æ„
            assert hasattr(model, 'decoder'), "Model missing decoder"
            assert hasattr(model, 'horizon_ff_layer'), "Model missing horizon_ff_layer"
            assert hasattr(model, 'config'), "Model missing config"
            
            # æµ‹è¯•æ¨¡å‹å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"  - Total parameters: {total_params:,}")
            print(f"  - Trainable parameters: {trainable_params:,}")
            
            # éªŒè¯æ¨¡å‹å¯ä»¥è®¾ç½®ä¸ºevalæ¨¡å¼
            model.eval()
            print(f"  - Model set to eval mode successfully")
            
            return True
        except Exception as e:
            print(f"âœ— Error creating TimesFM model: {e}")
            import traceback
            traceback.print_exc()
            return False

    def test_timesfm_forward_pass(self):
        """æµ‹è¯•TimesFMå‰å‘ä¼ æ’­"""
        print("\n--- Testing TimesFM Forward Pass ---")
        try:
            # ä½¿ç”¨ä¸input_ff_layeråŒ¹é…çš„é…ç½®
            config = TimesFmConfig(
                patch_length=32,  # ç¡®ä¿ä¸input_ff_layerçš„è¾“å…¥ç»´åº¦åŒ¹é…
                context_length=256,
                horizon_length=32,
                hidden_size=128,
                intermediate_size=256,
                num_hidden_layers=1,  # å…ˆç”¨è¾ƒå°‘çš„å±‚è¿›è¡Œæµ‹è¯•
                num_attention_heads=4,
                head_dim=32,
                freq_size=3,
                tolerance=1e-6,
                pad_val=0.0,
                use_positional_embedding=False,  # å…ˆç¦ç”¨ä½ç½®ç¼–ç 
            )
            
            print(f"  - Config: patch_length={config.patch_length}, "
                f"hidden_size={config.hidden_size}, "
                f"context_length={config.context_length}")
            
            model = TimesFmForPrediction(config)
            model.eval()
            
            # åˆ›å»ºä¸patch_lengthå¯¹é½çš„æµ‹è¯•è¾“å…¥
            batch_size = 2
            context_length = 256  # åº”è¯¥æ˜¯patch_lengthçš„å€æ•° (256 % 32 = 0)
            
            print(f"  - Testing with context_length={context_length}, patch_length={config.patch_length}")
            print(f"  - Number of patches: {context_length // config.patch_length}")
            
            test_data = []
            for i in range(batch_size):
                # åˆ›å»ºæ¨¡æ‹Ÿçš„æ—¶é—´åºåˆ—æ•°æ®
                ts_data = torch.randn(context_length)
                test_data.append(ts_data)
            
            print(f"  - Created test data: {len(test_data)} sequences of length {context_length}")
            
            # éªŒè¯è¾“å…¥æ•°æ®
            for i, ts in enumerate(test_data):
                assert ts.shape == (context_length,), f"Sequence {i} has wrong shape: {ts.shape}"
            
            # æµ‹è¯•å‰å‘ä¼ æ’­ - å°è¯•å¤šç§è°ƒç”¨æ–¹å¼
            print(f"  - Running forward pass...")
            output = None
            success_method = None
            
            with torch.no_grad():
                # æ–¹æ³•1: å°è¯•ä½¿ç”¨ __call__ æ–¹æ³•
                try:
                    print(f"    - Trying model(test_data)...")
                    output = model(test_data)
                    success_method = "model(test_data)"
                    print(f"    âœ“ Success with {success_method}")
                except Exception as e:
                    print(f"    âœ— model(test_data) failed: {e}")
                    import traceback
                    traceback.print_exc()
                
                # æ–¹æ³•2: å¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯• past_values å‚æ•°
                if output is None:
                    try:
                        print(f"    - Trying model(past_values=test_data)...")
                        output = model(past_values=test_data)
                        success_method = "model(past_values=test_data)"
                        print(f"    âœ“ Success with {success_method}")
                    except Exception as e:
                        print(f"    âœ— model(past_values=test_data) failed: {e}")
                
                # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯• tensor æ ¼å¼
                if output is None:
                    try:
                        print(f"    - Trying model(tensor)...")
                        test_tensor = torch.stack(test_data)  # è½¬æ¢ä¸ºtensor
                        output = model(test_tensor)
                        success_method = "model(tensor)"
                        print(f"    âœ“ Success with {success_method}")
                    except Exception as e:
                        print(f"    âœ— model(tensor) failed: {e}")
            
            if output is None:
                raise RuntimeError("All forward pass methods failed")
            
            print(f"âœ“ Forward pass successful using {success_method}")
            print(f"  - Input: {len(test_data)} sequences")
            print(f"  - Output type: {type(output)}")
            
            # éªŒè¯è¾“å‡º
            assert output is not None, "Output is None"
            
            if hasattr(output, 'mean_predictions') and output.mean_predictions is not None:
                print(f"  - Mean predictions shape: {output.mean_predictions.shape}")
                assert len(output.mean_predictions.shape) >= 2, "Mean predictions shape incorrect"
            
            if hasattr(output, 'full_predictions') and output.full_predictions is not None:
                print(f"  - Full predictions shape: {output.full_predictions.shape}")
            
            if hasattr(output, 'last_hidden_state') and output.last_hidden_state is not None:
                print(f"  - Hidden state shape: {output.last_hidden_state.shape}")
            
            # å¦‚æœoutputæ˜¯tensorï¼Œä¹Ÿæ‰“å°å…¶å½¢çŠ¶
            if torch.is_tensor(output):
                print(f"  - Tensor output shape: {output.shape}")
            
            return True
        except Exception as e:
            print(f"âœ— Error in forward pass: {e}")
            import traceback
            traceback.print_exc()
            return False

    def test_generation_interface(self):
        """æµ‹è¯•ç”Ÿæˆæ¥å£"""
        print("\n--- Testing Generation Interface ---")
        try:
            config = TimesFmConfig(
                patch_length=16,
                context_length=256,
                horizon_length=32,
                hidden_size=128,
                intermediate_size=128,
                num_hidden_layers=2,
                num_attention_heads=4,
                head_dim=32,
                freq_size=3
            )
            
            model = TimesFmForPrediction(config)
            model.eval()
            
            # æµ‹è¯•generateæ–¹æ³•
            batch_size = 2
            context_length = 256
            test_input = torch.randn(batch_size, context_length)
            
            successful_methods = []
            
            with torch.no_grad():
                # æµ‹è¯•tensorè¾“å…¥
                if hasattr(model, 'generate'):
                    try:
                        print(f"  - Testing generate with tensor input...")
                        output1 = model.generate(inputs=test_input)
                        print(f"    âœ“ Generate with tensor input successful")
                        print(f"      Output shape: {output1.shape if hasattr(output1, 'shape') else type(output1)}")
                        successful_methods.append("generate(tensor)")
                        
                        # éªŒè¯è¾“å‡º
                        assert output1 is not None, "Generate output is None"
                        if hasattr(output1, 'shape'):
                            assert len(output1.shape) >= 2, "Generate output shape incorrect"
                    except Exception as e:
                        print(f"    âœ— Generate with tensor input failed: {e}")
                
                # æµ‹è¯•åˆ—è¡¨è¾“å…¥
                if hasattr(model, 'generate'):
                    try:
                        print(f"  - Testing generate with list input...")
                        test_list = [test_input[i] for i in range(batch_size)]
                        output2 = model.generate(inputs=test_list, freq=[0, 1])
                        print(f"    âœ“ Generate with list input successful")
                        print(f"      Output shape: {output2.shape if hasattr(output2, 'shape') else type(output2)}")
                        successful_methods.append("generate(list)")
                        
                        # éªŒè¯è¾“å‡º
                        assert output2 is not None, "Generate output is None"
                    except Exception as e:
                        print(f"    âœ— Generate with list input failed: {e}")
                
                # å¦‚æœgenerateæ–¹æ³•ä¸å­˜åœ¨æˆ–éƒ½å¤±è´¥äº†ï¼Œå°è¯•ç›´æ¥è°ƒç”¨æ¨¡å‹
                if not successful_methods:
                    try:
                        print(f"  - Testing direct model call as fallback...")
                        test_list = [test_input[i] for i in range(batch_size)]
                        output3 = model(test_list)
                        print(f"    âœ“ Direct model call successful")
                        print(f"      Output type: {type(output3)}")
                        successful_methods.append("model(list)")
                    except Exception as e:
                        print(f"    âœ— Direct model call failed: {e}")
            
            if successful_methods:
                print(f"âœ“ Generation interface test successful")
                print(f"  - Working methods: {', '.join(successful_methods)}")
                return True
            else:
                print(f"âœ— No generation methods worked")
                return False
            
        except Exception as e:
            print(f"âœ— Error in generation interface: {e}")
            import traceback
            traceback.print_exc()
            return False

    def test_config_inheritance(self):
        """æµ‹è¯•é…ç½®ç»§æ‰¿å’Œå±æ€§è®¿é—®"""
        print("\n--- Testing Config Inheritance ---")
        try:
            config = TimesFmConfig(
                patch_length=16,
                context_length=128,
                hidden_size=64
            )
            
            # æµ‹è¯•çˆ¶ç±»å±æ€§
            print(f"  - Testing inherited attributes...")
            assert hasattr(config, 'model_type'), "Missing model_type"
            print(f"    - model_type: {config.model_type}")
            
            # æµ‹è¯•æ˜¯å¦æ­£ç¡®ç»§æ‰¿äº†PretrainedConfig
            from transformers import PretrainedConfig
            assert isinstance(config, PretrainedConfig), "Config not instance of PretrainedConfig"
            print(f"  - Config correctly inherits from PretrainedConfig")
            
            # æµ‹è¯•é…ç½®å¯ä»¥è¢«åºåˆ—åŒ–
            config_dict = config.to_dict()
            assert isinstance(config_dict, dict), "Config to_dict failed"
            print(f"  - Config serialization successful")
            print(f"    - Serialized keys: {len(config_dict)} items")
            
            # æµ‹è¯•ä»å­—å…¸é‡å»ºé…ç½®
            new_config = TimesFmConfig.from_dict(config_dict)
            assert new_config.patch_length == config.patch_length, "Config reconstruction failed"
            print(f"  - Config reconstruction from dict successful")
            
            return True
        except Exception as e:
            print(f"âœ— Error testing config inheritance: {e}")
            import traceback
            traceback.print_exc()
            return False

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ Starting TimesFM Integration Tests")
        print("=" * 60)
        
        tests = [
            ("Config Creation", self.test_config_creation),
            ("Config Inheritance", self.test_config_inheritance),
            ("Model Creation", self.test_timesfm_model_creation),
            ("Forward Pass", self.test_timesfm_forward_pass),
            ("Generation Interface", self.test_generation_interface),
        ]
        
        results = []
        for test_name, test_func in tests:
            try:
                print(f"\nğŸ§ª Running: {test_name}")
                result = test_func()
                results.append((test_name, result))
                if result:
                    print(f"âœ… {test_name} completed successfully")
                else:
                    print(f"âŒ {test_name} failed")
            except Exception as e:
                print(f"ğŸ’¥ {test_name} failed with exception: {e}")
                results.append((test_name, False))
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š Test Results Summary")
        print("=" * 60)
        
        passed = 0
        failed_tests = []
        for test_name, result in results:
            status = "âœ… PASS" if result else "âŒ FAIL"
            print(f"{status:10} {test_name}")
            if result:
                passed += 1
            else:
                failed_tests.append(test_name)
        
        success_rate = (passed / len(results)) * 100
        print(f"\nğŸ“ˆ Results: {passed}/{len(results)} tests passed ({success_rate:.1f}%)")
        
        if failed_tests:
            print(f"\nğŸ” Failed tests: {', '.join(failed_tests)}")
        
        # æ¸…ç†
        self.cleanup()
        
        return passed == len(results)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– TimesFM Integration Test Suite")
    print("=" * 60)
    
    # æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯
    print(f"Python version: {sys.version}")
    print(f"PyTorch version: {torch.__version__}")
    try:
        import transformers
        print(f"Transformers version: {transformers.__version__}")
    except ImportError:
        print("Transformers: Not installed")
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è¿è¡Œæµ‹è¯•
    test_runner = TimesFMIntegrationTest()
    success = test_runner.run_all_tests()
    
    if success:
        print("\nğŸ‰ All tests passed! TimesFM integration is working correctly.")
        sys.exit(0)
    else:
        print("\nğŸ’” Some tests failed! Please check the errors above.")
        sys.exit(1)

if __name__ == "__main__":
    main()