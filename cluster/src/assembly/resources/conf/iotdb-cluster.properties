#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

cluster_rpc_ip=127.0.0.1

# Note that the system will automatically create a heartbeat port for each of metadata service
# and data service. The default metadata heartbeat port is LOCAL_META_PORT + 1,
# The default data heartbeat port is LOCAL_DATA_PORT + 1.
# So when you configure these two items, pay attention to reserve a port for heartbeat service.

# port for metadata service
internal_meta_port=9003

# port for data service
internal_data_port=40010

# port for client service
cluster_rpc_port=55560

# comma-separated {IP/DOMAIN}:meta_port:data_port pairs
# when used by start-node.sh(.bat), this configuration means the nodes that will form the initial
# cluster, some every node that use start-node.sh(.bat) should have the SAME SEED_NODES, or the
# building of the initial cluster will fail. WARNING: if the initial cluster is built, this
# should not be changed before the environment is cleaned.
# when used by add-node.sh(.bat), this means the nodes to which that the application of joining
# the cluster will be sent, as all nodes can respond to a request, this configuration can be any
# nodes that already in the cluster, unnecessary to be the nodes that were used to build the
# initial cluster by start-node.sh(.bat). Several nodes will be picked randomly to send the
# request, the number of nodes picked depends on the number of retries.
seed_nodes=127.0.0.1:9003:40010,127.0.0.1:9005:40012,127.0.0.1:9007:40014

# whether to use thrift compressed protocol
# WARNING: this must be consistent across all nodes in the cluster
rpc_thrift_compression_enable=true

# max client connections created by thrift
# this configuration applies separately to data/meta/client connections and thus does not control
# the number of global connections
max_concurrent_client_num=10000

# number of replications for one partition
default_replica_num=2

# cluster name to identify different clusters
# all node's cluster_name in one cluster are the same
cluster_name=default

# connection time out (ms) among raft nodes
connection_timeout_ms=20000

# write operation timeout threshold (ms), this is only for internal communications,
# not for the whole operation.
write_operation_timeout_ms=30000

# read operation timeout threshold (ms), this is only for internal communications,
# not for the whole operation.
read_operation_timeout_ms=30000

# when the logs size larger than this, we actually delete snapshoted logs, the unit is bytes
max_unsnapshoted_log_size=134217728

# whether to use batch append entries in log catch up
use_batch_in_catch_up=true

# minimum number of committed logs in memory, after each log deletion, at most such number of logs
# will remain in memory. Increasing the number will reduce the chance to use snapshot in catch-ups,
# but will also increase memory footprint
min_num_of_logs_in_mem=100

# maximum number of committed logs in memory, when reached, a log deletion will be triggered.
# Increasing the number will reduce the chance to use snapshot in catch-ups, but will also increase
# memory footprint
max_num_of_logs_in_mem=1000

# deletion check period of the submitted log
log_deletion_check_interval_second=60

# Whether creating schema automatically is enabled, this will replace the one in iotdb-engine.properties
enable_auto_create_schema=true

# consistency level, now three consistency levels are supported: strong, mid and weak.
# Strong consistency means the server will first try to synchronize with the leader to get the
# newest meta data, if failed(timeout), directly report an error to the user;
# While mid consistency means the server will first try to synchronize with the leader,
# but if failed(timeout), it will give up and just use current data it has cached before;
# Weak consistency do not synchronize with the leader and simply use the local data
consistency_level=mid

# is raft log persistence enabled
is_enable_raft_log_persistence=true

# When a certain amount of raft log is reached, it will be flushed to disk
# It is possible to lose at most flush_wal_threshold operations
flush_raft_log_threshold=10000

# The cycle when raft log is periodically forced to be written to disk(in milliseconds)
# If force_raft_log_period_in_ms = 0 it means force insert ahead log to be written to disk after each refreshment
# Set this parameter to 0 may slow down the ingestion on slow disk.
force_raft_log_period_in_ms=10

raft_log_buffer_size=16777216





